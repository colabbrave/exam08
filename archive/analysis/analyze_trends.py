#!/usr/bin/env python3
"""
å“è³ªåˆ†æ•¸è®Šå‹•è¶¨å‹¢åˆ†æå·¥å…·
åˆ†ææœƒè­°è¨˜éŒ„å„ªåŒ–ç³»çµ±çš„æ•ˆæœ
"""

import json
import os
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import pandas as pd
from datetime import datetime

def analyze_iteration_scores(meeting_folder):
    """åˆ†æå–®æ¬¡æœƒè­°çš„ç–Šä»£åˆ†æ•¸è®ŠåŒ–"""
    iterations_dir = Path(f'results/iterations/{meeting_folder}')
    
    if not iterations_dir.exists():
        return [], {}
    
    scores = []
    score_files = sorted([f for f in iterations_dir.glob('iteration_*_scores.json')], 
                        key=lambda x: int(x.stem.split('_')[1]))
    
    for score_file in score_files:
        try:
            with open(score_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                iteration = int(score_file.stem.split('_')[1])
                
                # æå–ç¸½åˆ†
                total_score = None
                if 'scores' in data and 'overall_score' in data['scores']:
                    total_score = data['scores']['overall_score']
                elif 'overall_quality' in data:
                    total_score = data['overall_quality']
                elif 'quality_score' in data:
                    total_score = data['quality_score']
                
                if total_score is not None:
                    scores.append((iteration, total_score))
                    
        except Exception as e:
            print(f'è­¦å‘Š: ç„¡æ³•è®€å– {score_file}: {e}')
    
    # åˆ†æçµ±è¨ˆè³‡æ–™
    if not scores:
        return [], {}
    
    iterations, score_values = zip(*scores)
    stats = {
        'total_iterations': len(scores),
        'initial_score': score_values[0],
        'final_score': score_values[-1],
        'max_score': max(score_values),
        'min_score': min(score_values),
        'max_iteration': iterations[score_values.index(max(score_values))],
        'min_iteration': iterations[score_values.index(min(score_values))],
        'improvement': score_values[-1] - score_values[0],
        'volatility': np.std(score_values),
        'trend': 'improving' if score_values[-1] > score_values[0] else 'declining'
    }
    
    return scores, stats

def create_trend_visualization():
    """å‰µå»ºå“è³ªè¶¨å‹¢è¦–è¦ºåŒ–åœ–è¡¨"""
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('æœƒè­°è¨˜éŒ„å„ªåŒ–ç³»çµ± - å“è³ªåˆ†æ•¸è®Šå‹•è¶¨å‹¢åˆ†æ', fontsize=16, fontweight='bold')
    
    meetings = ['ç¬¬671æ¬¡å¸‚æ”¿æœƒè­°114å¹´5æœˆ13æ—¥é€å­—ç¨¿', 'ç¬¬672æ¬¡å¸‚æ”¿æœƒè­°114å¹´5æœˆ20æ—¥é€å­—ç¨¿']
    colors = ['#2E86AB', '#A23B72']
    
    all_stats = {}
    
    for idx, meeting in enumerate(meetings):
        scores, stats = analyze_iteration_scores(meeting)
        all_stats[meeting] = stats
        
        if not scores:
            continue
            
        iterations, score_values = zip(*scores)
        
        # ä¸»è¦è¶¨å‹¢åœ–
        ax = axes[0, idx]
        ax.plot(iterations, score_values, color=colors[idx], linewidth=2, marker='o', markersize=4)
        ax.set_title(f'{meeting[:8]}...', fontsize=12, fontweight='bold')
        ax.set_xlabel('ç–Šä»£æ¬¡æ•¸')
        ax.set_ylabel('å“è³ªåˆ†æ•¸')
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, max(1, max(score_values) * 1.1))
        
        # æ·»åŠ é—œéµé»æ¨™è¨»
        max_idx = score_values.index(max(score_values))
        ax.annotate(f'æœ€é«˜: {max(score_values):.4f}\n(ç¬¬{iterations[max_idx]}è¼ª)', 
                   xy=(iterations[max_idx], max(score_values)),
                   xytext=(10, 10), textcoords='offset points',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
    
    # æ¯”è¼ƒçµ±è¨ˆåœ–
    ax = axes[1, 0]
    meeting_names = [f'ç¬¬{671+i}æ¬¡' for i in range(len(all_stats))]
    metrics = ['initial_score', 'final_score', 'max_score']
    metric_labels = ['åˆå§‹åˆ†æ•¸', 'æœ€çµ‚åˆ†æ•¸', 'æœ€é«˜åˆ†æ•¸']
    
    x = np.arange(len(meeting_names))
    width = 0.25
    
    for i, (metric, label) in enumerate(zip(metrics, metric_labels)):
        values = [all_stats[meeting][metric] for meeting in meetings if meeting in all_stats]
        ax.bar(x + i*width, values, width, label=label, alpha=0.8)
    
    ax.set_xlabel('æœƒè­°')
    ax.set_ylabel('åˆ†æ•¸')
    ax.set_title('æœƒè­°åˆ†æ•¸æ¯”è¼ƒ')
    ax.set_xticks(x + width)
    ax.set_xticklabels(meeting_names)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # æ”¹å–„æ•ˆæœåˆ†æ
    ax = axes[1, 1]
    improvements = [all_stats[meeting]['improvement'] for meeting in meetings if meeting in all_stats]
    volatilities = [all_stats[meeting]['volatility'] for meeting in meetings if meeting in all_stats]
    
    colors_scatter = ['green' if imp > 0 else 'red' for imp in improvements]
    ax.scatter(improvements, volatilities, c=colors_scatter, s=100, alpha=0.7)
    
    for i, meeting in enumerate([m for m in meetings if m in all_stats]):
        ax.annotate(f'ç¬¬{671+i}æ¬¡', (improvements[i], volatilities[i]), 
                   xytext=(5, 5), textcoords='offset points')
    
    ax.set_xlabel('æ”¹å–„å¹…åº¦ (æœ€çµ‚-åˆå§‹)')
    ax.set_ylabel('åˆ†æ•¸æ³¢å‹•æ€§ (æ¨™æº–å·®)')
    ax.set_title('æ”¹å–„æ•ˆæœ vs ç©©å®šæ€§')
    ax.grid(True, alpha=0.3)
    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig('results/quality_trends_analysis.png', dpi=300, bbox_inches='tight')
    print("åœ–è¡¨å·²ä¿å­˜è‡³: results/quality_trends_analysis.png")
    
    return all_stats

def generate_analysis_report():
    """ç”Ÿæˆè©³ç´°çš„åˆ†æå ±å‘Š"""
    print("=" * 80)
    print("æœƒè­°è¨˜éŒ„å„ªåŒ–ç³»çµ± - å“è³ªåˆ†æ•¸è®Šå‹•è¶¨å‹¢åˆ†æå ±å‘Š")
    print("=" * 80)
    print(f"åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    meetings = ['ç¬¬671æ¬¡å¸‚æ”¿æœƒè­°114å¹´5æœˆ13æ—¥é€å­—ç¨¿', 'ç¬¬672æ¬¡å¸‚æ”¿æœƒè­°114å¹´5æœˆ20æ—¥é€å­—ç¨¿']
    
    for i, meeting in enumerate(meetings):
        print(f"{i+1}. {meeting}")
        print("-" * 60)
        
        scores, stats = analyze_iteration_scores(meeting)
        
        if not scores:
            print("   âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†æ•¸è³‡æ–™")
            continue
        
        print(f"   ğŸ“Š ç–Šä»£çµ±è¨ˆ:")
        print(f"      â€¢ ç¸½ç–Šä»£æ¬¡æ•¸: {stats['total_iterations']} è¼ª")
        print(f"      â€¢ åˆå§‹åˆ†æ•¸: {stats['initial_score']:.4f}")
        print(f"      â€¢ æœ€çµ‚åˆ†æ•¸: {stats['final_score']:.4f}")
        print(f"      â€¢ æœ€é«˜åˆ†æ•¸: {stats['max_score']:.4f} (ç¬¬{stats['max_iteration']}è¼ª)")
        print(f"      â€¢ æœ€ä½åˆ†æ•¸: {stats['min_score']:.4f} (ç¬¬{stats['min_iteration']}è¼ª)")
        
        print(f"   ğŸ“ˆ æ•ˆæœè©•ä¼°:")
        improvement = stats['improvement']
        if improvement > 0:
            print(f"      â€¢ âœ… æ•´é«”æ”¹å–„: +{improvement:.4f} ({improvement/stats['initial_score']*100:.1f}%)")
        else:
            print(f"      â€¢ âŒ æ•´é«”ä¸‹é™: {improvement:.4f} ({improvement/stats['initial_score']*100:.1f}%)")
        
        print(f"      â€¢ ç©©å®šæ€§ (æ³¢å‹•åº¦): {stats['volatility']:.4f}")
        print(f"      â€¢ è¶¨å‹¢æ–¹å‘: {'ğŸ“ˆ ä¸Šå‡' if stats['trend'] == 'improving' else 'ğŸ“‰ ä¸‹é™'}")
        
        # åˆ†æç•°å¸¸æƒ…æ³
        if stats['total_iterations'] > 50:
            print(f"      â€¢ âš ï¸  ç•°å¸¸: ç–Šä»£æ¬¡æ•¸éå¤š ({stats['total_iterations']} è¼ª)")
            print(f"         å¯èƒ½åŸå› : early stopping æ©Ÿåˆ¶æœªæ­£å¸¸é‹ä½œ")
        
        if stats['volatility'] > 0.1:
            print(f"      â€¢ âš ï¸  ç•°å¸¸: åˆ†æ•¸æ³¢å‹•éå¤§ (Ïƒ={stats['volatility']:.4f})")
            print(f"         å¯èƒ½åŸå› : ç­–ç•¥å„ªåŒ–ä¸ç©©å®šæˆ–è©•ä¼°æŒ‡æ¨™æ³¢å‹•")
        
        print()
    
    # ç”Ÿæˆå»ºè­°
    print("ğŸ” ç³»çµ±å„ªåŒ–å»ºè­°:")
    print("-" * 60)
    
    all_meetings_stats = {}
    for meeting in meetings:
        _, stats = analyze_iteration_scores(meeting)
        if stats:
            all_meetings_stats[meeting] = stats
    
    if len(all_meetings_stats) >= 2:
        avg_iterations = np.mean([s['total_iterations'] for s in all_meetings_stats.values()])
        avg_improvement = np.mean([s['improvement'] for s in all_meetings_stats.values()])
        
        if avg_iterations > 30:
            print("   1. âš™ï¸  èª¿æ•´ early stopping åƒæ•¸ï¼Œé¿å…éåº¦ç–Šä»£")
            print("      â€¢ é™ä½ patience å€¼ (å»ºè­°: 3-5)")
            print("      â€¢ æé«˜ min_improvement é–¾å€¼ (å»ºè­°: 0.01)")
        
        if avg_improvement < 0:
            print("   2. ğŸ¯ å„ªåŒ–ç­–ç•¥é¸æ“‡æ©Ÿåˆ¶")
            print("      â€¢ æª¢æŸ¥ LLM æ”¹é€²å»ºè­°çš„å“è³ª")
            print("      â€¢ èª¿æ•´ç­–ç•¥æ¬Šé‡å’Œçµ„åˆé‚è¼¯")
        
        max_volatility = max([s['volatility'] for s in all_meetings_stats.values()])
        if max_volatility > 0.1:
            print("   3. ğŸ“Š ç©©å®šè©•ä¼°æŒ‡æ¨™")
            print("      â€¢ æª¢æŸ¥ BERTScore è¨ˆç®—çš„ä¸€è‡´æ€§")
            print("      â€¢ è€ƒæ…®ä½¿ç”¨å¤šæŒ‡æ¨™å¹³å‡ä¾†æ¸›å°‘æ³¢å‹•")
    
    print()
    
    # å‰µå»ºè¦–è¦ºåŒ–
    stats_dict = create_trend_visualization()
    
    return stats_dict

if __name__ == "__main__":
    # ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨
    os.makedirs('results', exist_ok=True)
    
    # åŸ·è¡Œåˆ†æ
    stats = generate_analysis_report()
    
    # ä¿å­˜çµ±è¨ˆè³‡æ–™åˆ° JSON
    with open('results/trend_analysis_stats.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print("âœ… åˆ†æå®Œæˆï¼")
    print("ğŸ“ çµæœæª”æ¡ˆ:")
    print("   â€¢ results/quality_trends_analysis.png (è¦–è¦ºåŒ–åœ–è¡¨)")
    print("   â€¢ results/trend_analysis_stats.json (çµ±è¨ˆè³‡æ–™)")
