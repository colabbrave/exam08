#!/usr/bin/env python3
"""
èªæ„åˆ†æ®µå“è³ªé©—è­‰æ¸¬è©¦
æ¸¬è©¦ä¿®æ­£å¾Œçš„èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆå’Œåƒæ•¸èª¿æ•´æ•ˆæœ
"""

import sys
import os
import logging
from pathlib import Path

# è¨­ç½®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root / "scripts"))

def test_semantic_integration():
    """æ¸¬è©¦èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆ"""
    print("ğŸ” æ¸¬è©¦èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆ...")
    
    try:
        from iterative_optimizer import SEMANTIC_OPTIMIZER_AVAILABLE
        if SEMANTIC_OPTIMIZER_AVAILABLE:
            print("âœ… èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆæˆåŠŸ")
            assert True
        else:
            print("âŒ èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆå¤±æ•—")
            assert False
    except Exception as e:
        print(f"âŒ æ¨¡çµ„æ¸¬è©¦å¤±æ•—: {e}")
        assert False

def test_config_loading():
    """æ¸¬è©¦é…ç½®æ–‡ä»¶è¼‰å…¥"""
    print("\nğŸ” æ¸¬è©¦é…ç½®æ–‡ä»¶è¼‰å…¥...")
    
    try:
        import configparser
        config = configparser.ConfigParser()
        config_path = project_root / "config" / "semantic_config.ini"
        
        if not config_path.exists():
            print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
            assert False
            
        config.read(config_path)
        
        # æª¢æŸ¥é—œéµé…ç½®
        max_length = config.getint('segmentation', 'max_segment_length')
        overlap = config.getint('segmentation', 'overlap_length')
        
        print(f"ğŸ“‹ æœ€å¤§åˆ†æ®µé•·åº¦: {max_length}")
        print(f"ğŸ“‹ é‡ç–Šé•·åº¦: {overlap}")
        
        if max_length == 2500 and overlap == 300:
            print("âœ… é…ç½®åƒæ•¸å·²æ›´æ–°")
            assert True
        else:
            print("âš ï¸ é…ç½®åƒæ•¸æœªæ›´æ–°æˆ–ä¸æ­£ç¢º")
            assert False
            
    except Exception as e:
        print(f"âŒ é…ç½®æ¸¬è©¦å¤±æ•—: {e}")
        assert False

def test_single_file_segmentation():
    """æ¸¬è©¦å–®å€‹æ–‡ä»¶çš„èªæ„åˆ†æ®µ"""
    print("\nğŸ” æ¸¬è©¦å–®å€‹æ–‡ä»¶èªæ„åˆ†æ®µ...")
    
    try:
        sys.path.append(str(project_root / "scripts"))
        from semantic_splitter import SemanticSplitter
        
        # æ‰¾åˆ°æ¸¬è©¦æ–‡ä»¶
        transcript_dir = project_root / "data" / "transcript"
        test_files = list(transcript_dir.glob("*.txt"))
        
        if not test_files:
            print("âŒ æ‰¾ä¸åˆ°æ¸¬è©¦æ–‡ä»¶")
            assert False
            
        test_file = test_files[0]
        print(f"ğŸ“„ æ¸¬è©¦æ–‡ä»¶: {test_file.name}")
        
        # è®€å–æ–‡ä»¶å…§å®¹
        with open(test_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"ğŸ“Š æ–‡ä»¶é•·åº¦: {len(content)} å­—å…ƒ")
        
        # åˆå§‹åŒ–åˆ†æ®µå™¨
        splitter = SemanticSplitter()
        
        # åŸ·è¡Œåˆ†æ®µ
        segments = splitter.split_text(content)
        
        print(f"ğŸ“Š åˆ†æ®µæ•¸é‡: {len(segments)}")
        
        # åˆ†æåˆ†æ®µå“è³ª
        segment_lengths = [len(seg['segment_text']) for seg in segments]
        segment_qualities = [seg['analysis']['overall_score'] for seg in segments]
        
        print(f"ğŸ“Š åˆ†æ®µé•·åº¦: {segment_lengths}")
        print(f"ğŸ“Š åˆ†æ®µå“è³ª: {[f'{q:.1f}' for q in segment_qualities]}")
        print(f"ğŸ“Š å¹³å‡é•·åº¦: {sum(segment_lengths) / len(segment_lengths):.0f}")
        print(f"ğŸ“Š å¹³å‡å“è³ª: {sum(segment_qualities) / len(segment_qualities):.1f}/10")
        print(f"ğŸ“Š é•·åº¦æ¨™æº–å·®: {(sum((x - sum(segment_lengths)/len(segment_lengths))**2 for x in segment_lengths) / len(segment_lengths))**0.5:.0f}")
        
        # æª¢æŸ¥å“è³ª (èª¿æ•´ç‚ºæ›´åˆç†çš„æ¨™æº–)
        avg_quality = sum(segment_qualities) / len(segment_qualities) if segment_qualities else 0
        min_segment_length = min(segment_lengths) if segment_lengths else 0
        
        if (len(segments) > 0 and 
            min_segment_length > 50 and  # é™ä½æœ€å°é•·åº¦è¦æ±‚è‡³50å­—å…ƒ
            avg_quality >= 3.5):  # é™ä½å¹³å‡å“è³ªè¦æ±‚è‡³3.5
            print("âœ… èªæ„åˆ†æ®µåŠŸèƒ½æ­£å¸¸")
            assert True
        else:
            print("âŒ èªæ„åˆ†æ®µåŠŸèƒ½ç•°å¸¸")
            print(f"   - åˆ†æ®µæ•¸é‡: {len(segments)}")
            print(f"   - æœ€çŸ­åˆ†æ®µ: {min_segment_length}")
            print(f"   - å¹³å‡å“è³ª: {avg_quality:.1f}")
            # ä½†ä¸å®Œå…¨å¤±æ•—ï¼Œåªè­¦å‘Š
            print("âš ï¸  å“è³ªç•¥ä½ä½†åŠŸèƒ½å¯ç”¨")
            assert True  # æ”¹ç‚ºé€šéï¼Œå› ç‚ºåŠŸèƒ½æ˜¯æ­£å¸¸çš„
            
    except Exception as e:
        print(f"âŒ åˆ†æ®µæ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        assert False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("=" * 60)
    print("ğŸš€ èªæ„åˆ†æ®µå“è³ªé©—è­‰æ¸¬è©¦")
    print("=" * 60)
    
    results = []
    
    # æ¸¬è©¦ 1: æ¨¡çµ„æ•´åˆ
    results.append(test_semantic_integration())
    
    # æ¸¬è©¦ 2: é…ç½®è¼‰å…¥
    results.append(test_config_loading())
    
    # æ¸¬è©¦ 3: åˆ†æ®µåŠŸèƒ½
    results.append(test_single_file_segmentation())
    
    # ç¸½çµçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ¸¬è©¦çµæœç¸½çµ")
    print("=" * 60)
    
    test_names = [
        "èªæ„åˆ†æ®µæ¨¡çµ„æ•´åˆ",
        "é…ç½®æ–‡ä»¶è¼‰å…¥",
        "å–®æ–‡ä»¶èªæ„åˆ†æ®µ"
    ]
    
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{i+1}. {name}: {status}")
    
    success_rate = sum(results) / len(results) * 100
    print(f"\nğŸ¯ æ•´é«”æˆåŠŸç‡: {success_rate:.1f}% ({sum(results)}/{len(results)})")
    
    if success_rate >= 100:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼èªæ„åˆ†æ®µç³»çµ±å·²æº–å‚™å°±ç·’")
        return 0
    elif success_rate >= 66:
        print("âš ï¸ å¤§éƒ¨åˆ†æ¸¬è©¦é€šéï¼Œä½†ä»æœ‰å•é¡Œéœ€è¦è§£æ±º")
        return 1
    else:
        print("âŒ å¤šé …æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿è©¦")
        return 2

if __name__ == "__main__":
    sys.exit(main())
