#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èªæ„åˆ†æ®µåŠŸèƒ½æ•´åˆæ¸¬è©¦è…³æœ¬
æ¸¬è©¦èªæ„åˆ†æ®µèˆ‡ç¾æœ‰å„ªåŒ–æµç¨‹çš„å…¼å®¹æ€§
"""

import os
import sys
import logging
import time
from pathlib import Path
from typing import Dict, List, Any
import pytest

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å°å…¥ç›¸é—œæ¨¡çµ„
try:
    from scripts.optimize_meeting_minutes import MeetingOptimizer
    from scripts.semantic_splitter import SemanticSplitter
    from scripts.segment_quality_eval import SegmentQualityEvaluator
    from scripts.semantic_meeting_processor import SemanticMeetingProcessor
    print("âœ“ æˆåŠŸå°å…¥æ‰€æœ‰èªæ„åˆ†æ®µæ¨¡çµ„")
except ImportError as e:
    print(f"âœ— å°å…¥æ¨¡çµ„å¤±æ•—: {e}")
    sys.exit(1)

def setup_logging():
    """è¨­ç½®æ—¥èªŒ"""
    log_dir = project_root / "logs"
    log_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_dir / f"semantic_integration_test_{int(time.time())}.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("SemanticIntegrationTest")

def create_test_data() -> Dict[str, Any]:
    """å‰µå»ºæ¸¬è©¦æ•¸æ“š"""
    # å‰µå»ºä¸€å€‹å¤§å‹é€å­—ç¨¿æ¨£æœ¬
    large_transcript = """
å°ä¸­å¸‚æ”¿åºœç¬¬150æ¬¡å¸‚æ”¿æœƒè­°è¨˜éŒ„

æœƒè­°æ™‚é–“ï¼š112å¹´6æœˆ15æ—¥ä¸‹åˆ2æ™‚
æœƒè­°åœ°é»ï¼šå°ä¸­å¸‚æ”¿åºœæœƒè­°å®¤
ä¸»å¸­ï¼šå¸‚é•·ç›§ç§€ç‡•
å‡ºå¸­äººå“¡ï¼šå‰¯å¸‚é•·ã€å„å±€è™•é¦–é•·

æœƒè­°é–‹å§‹ï¼š
å¸‚é•·è‡´è©ï¼šå„ä½åŒä»å¤§å®¶å¥½ï¼Œä»Šå¤©çš„å¸‚æ”¿æœƒè­°ä¸»è¦è¨è«–å¹¾å€‹é‡è¦è­°é¡Œ...

ç¬¬ä¸€æ¡ˆï¼šéƒ½å¸‚è¨ˆç•«é€šç›¤æª¢è¨æ¡ˆ
ææ¡ˆæ©Ÿé—œï¼šéƒ½å¸‚ç™¼å±•å±€
æ¡ˆç”±ï¼šè¾¦ç†å°ä¸­å¸‚éƒ½å¸‚è¨ˆç•«é€šç›¤æª¢è¨ï¼Œå»ºè«‹å¯©è­°ã€‚
èªªæ˜ï¼š
ä¸€ã€æœ¬æ¡ˆä¿‚ä¾éƒ½å¸‚è¨ˆç•«æ³•ç¬¬26æ¢è¦å®šï¼Œæ¯äº”å¹´é€šç›¤æª¢è¨ä¸€æ¬¡ã€‚
äºŒã€æª¢è¨ç¯„åœåŒ…å«å…¨å¸‚29å€‹éƒ½å¸‚è¨ˆç•«å€ã€‚
ä¸‰ã€é è¨ˆéœ€æ™‚18å€‹æœˆå®Œæˆã€‚

å„å±€è™•æ„è¦‹ï¼š
äº¤é€šå±€ï¼šå»ºè­°åŠ å¼·äº¤é€šç³»çµ±æ•´é«”è¦åŠƒï¼Œç‰¹åˆ¥æ˜¯å¤§çœ¾é‹è¼¸ç³»çµ±çš„éŠœæ¥ã€‚
ç’°ä¿å±€ï¼šæ‡‰ç´å…¥ç’°å¢ƒå½±éŸ¿è©•ä¼°ï¼Œç¢ºä¿éƒ½å¸‚ç™¼å±•èˆ‡ç’°å¢ƒä¿è­·å¹³è¡¡ã€‚
æ°´åˆ©å±€ï¼šéœ€è€ƒé‡æ’æ°´ç³»çµ±å®¹é‡ï¼Œé¿å…éƒ½å¸‚åŒ–é€ æˆæ·¹æ°´å•é¡Œã€‚

è¨è«–ï¼š
å¸‚é•·ï¼šé€™å€‹æ¡ˆå­éå¸¸é‡è¦ï¼Œé—œä¿‚åˆ°å°ä¸­å¸‚æœªä¾†20å¹´çš„ç™¼å±•ã€‚è«‹éƒ½ç™¼å±€å†è©³ç´°èªªæ˜æª¢è¨é‡é»ã€‚
éƒ½ç™¼å±€é•·ï¼šä¸»è¦æª¢è¨é …ç›®åŒ…æ‹¬ï¼š
1. åœŸåœ°ä½¿ç”¨åˆ†å€èª¿æ•´
2. å…¬å…±è¨­æ–½ç”¨åœ°æª¢è¨
3. äº¤é€šç³»çµ±è¦åŠƒ
4. ç’°å¢ƒå“è³ªç¶­è­·
5. ç”¢æ¥­ç™¼å±•ç©ºé–“è¦åŠƒ

æ±ºè­°ï¼š
ä¸€ã€åŸå‰‡åŒæ„è¾¦ç†éƒ½å¸‚è¨ˆç•«é€šç›¤æª¢è¨ã€‚
äºŒã€è«‹éƒ½ç™¼å±€æ–¼ä¸€å€‹æœˆå…§æå‡ºè©³ç´°åŸ·è¡Œè¨ˆç•«ã€‚
ä¸‰ã€æˆç«‹è·¨å±€è™•å·¥ä½œå°çµ„ï¼Œç”±å‰¯å¸‚é•·æ“”ä»»å¬é›†äººã€‚

ç¬¬äºŒæ¡ˆï¼šæ™ºæ…§åŸå¸‚å»ºè¨­è¨ˆç•«
ææ¡ˆæ©Ÿé—œï¼šç ”è€ƒæœƒ
æ¡ˆç”±ï¼šæ¨å‹•å°ä¸­æ™ºæ…§åŸå¸‚å»ºè¨­ï¼Œå»ºè«‹å¯©è­°ã€‚
èªªæ˜ï¼š
ä¸€ã€é…åˆåœ‹å®¶æ•¸ä½è½‰å‹æ”¿ç­–ï¼Œæ¨å‹•æ™ºæ…§åŸå¸‚å»ºè¨­ã€‚
äºŒã€è¨ˆç•«æœŸç¨‹ç‚º5å¹´ï¼Œé ç®—éœ€æ±‚ç´„50å„„å…ƒã€‚
ä¸‰ã€ä¸»è¦å»ºè¨­é …ç›®åŒ…æ‹¬æ™ºæ…§äº¤é€šã€æ™ºæ…§æ•™è‚²ã€æ™ºæ…§é†«ç™‚ã€æ™ºæ…§æ²»ç†ç­‰ã€‚

å„å±€è™•æ„è¦‹ï¼š
è³‡è¨Šå±€ï¼šæŠ€è¡“é¢å¯è¡Œï¼Œä½†éœ€è¦å¤§é‡å°ˆæ¥­äººåŠ›ã€‚
è²¡æ”¿å±€ï¼šé ç®—é¾å¤§ï¼Œå»ºè­°åˆ†æœŸåˆ†å¹´ç·¨åˆ—ã€‚
æ•™è‚²å±€ï¼šæ™ºæ…§æ•™è‚²éƒ¨åˆ†ï¼Œéœ€è¦æ•™å¸«åŸ¹è¨“é…å¥—æªæ–½ã€‚

è¨è«–ï¼š
å¸‚é•·ï¼šæ™ºæ…§åŸå¸‚æ˜¯æœªä¾†è¶¨å‹¢ï¼Œä½†è¦å‹™å¯¦æ¨å‹•ï¼Œä¸èƒ½ç‚ºäº†æ™ºæ…§è€Œæ™ºæ…§ã€‚
ç ”è€ƒæœƒä¸»å§”ï¼šæˆ‘å€‘å·²ç¶“åƒè€ƒå…¶ä»–ç¸£å¸‚ç¶“é©—ï¼Œä¸¦è€ƒé‡å°ä¸­å¸‚çš„ç‰¹è‰²ã€‚

æ±ºè­°ï¼š
ä¸€ã€åŸå‰‡åŒæ„æ¨å‹•æ™ºæ…§åŸå¸‚å»ºè¨­è¨ˆç•«ã€‚
äºŒã€è«‹ç ”è€ƒæœƒä¿®æ­£è¨ˆç•«å…§å®¹ï¼Œé™ä½é ç®—éœ€æ±‚ã€‚
ä¸‰ã€å„ªå…ˆæ¨å‹•æ™ºæ…§äº¤é€šå’Œæ™ºæ…§æ²»ç†é …ç›®ã€‚

ç¬¬ä¸‰æ¡ˆï¼šç¤¾æœƒä½å®…èˆˆå»ºè¨ˆç•«
ææ¡ˆæ©Ÿé—œï¼šä½å®…ç™¼å±•å·¥ç¨‹è™•
æ¡ˆç”±ï¼šèˆˆå»ºç¤¾æœƒä½å®…ï¼Œè§£æ±ºé’å¹´å±…ä½å•é¡Œï¼Œå»ºè«‹å¯©è­°ã€‚
èªªæ˜ï¼š
ä¸€ã€è¦åŠƒåœ¨åŒ—å±¯ã€è¥¿å±¯ã€å¤ªå¹³ç­‰å€èˆˆå»ºç¤¾æœƒä½å®…ã€‚
äºŒã€é è¨ˆèˆˆå»º3000æˆ¶ï¼Œæä¾›é’å¹´åŠå¼±å‹¢æ—ç¾¤ç§Ÿç”¨ã€‚
ä¸‰ã€çµåˆæ‰˜è‚²ã€é•·ç…§ç­‰ç¤¾æœƒç¦åˆ©è¨­æ–½ã€‚

å„å±€è™•æ„è¦‹ï¼š
ç¤¾æœƒå±€ï¼šæ”¯æŒçµåˆç¤¾ç¦è¨­æ–½ï¼Œä½†éœ€è¦ç‡Ÿé‹ç¶“è²»ã€‚
æ•™è‚²å±€ï¼šå»ºè­°è¦åŠƒå­¸æ ¡ç”¨åœ°ï¼Œè§£æ±ºå°±å­¸å•é¡Œã€‚
è¡›ç”Ÿå±€ï¼šå¯çµåˆç¤¾å€å¥åº·ä¸­å¿ƒè¨­ç½®ã€‚

è¨è«–ï¼š
å¸‚é•·ï¼šç¤¾æœƒä½å®…ä¸åªæ˜¯è“‹æˆ¿å­ï¼Œè¦ç‡Ÿé€ ç¤¾å€ç”Ÿæ´»åœˆã€‚
ä½å®…è™•é•·ï¼šæˆ‘å€‘å·²ç¶“è¦åŠƒå®Œæ•´çš„ç¤¾å€æœå‹™æ©Ÿèƒ½ã€‚

æ±ºè­°ï¼š
ä¸€ã€åŒæ„èˆˆå»ºç¤¾æœƒä½å®…è¨ˆç•«ã€‚
äºŒã€å„ªå…ˆæ¨å‹•åŒ—å±¯å€åŸºåœ°ã€‚
ä¸‰ã€è«‹å„ç›¸é—œå±€è™•å”åŠ©é…åˆã€‚

æ•£æœƒæ™‚é–“ï¼šä¸‹åˆ5æ™‚30åˆ†
""" * 3  # é‡è¤‡3æ¬¡ä»¥å¢åŠ é•·åº¦

    return {
        "transcript": large_transcript,
        "transcript_file": "ç¬¬150æ¬¡å¸‚æ”¿æœƒè­°112å¹´6æœˆ15æ—¥.txt",
        "reference": large_transcript[:2000] + "...",  # ç°¡åŒ–çš„åƒè€ƒ
        "reference_file": "reference_150.txt"
    }

def test_semantic_splitter():
    """æ¸¬è©¦èªæ„åˆ†æ®µå™¨"""
    logger = logging.getLogger("SemanticSplitterTest")
    logger.info("=== æ¸¬è©¦èªæ„åˆ†æ®µå™¨ ===")
    
    try:
        splitter = SemanticSplitter()
        
        test_data = create_test_data()
        transcript = test_data["transcript"]
        
        logger.info(f"åŸå§‹æ–‡æœ¬é•·åº¦: {len(transcript)} å­—å…ƒ")
        
        # åŸ·è¡Œåˆ†æ®µ
        segments = splitter.split_text(transcript)
        
        logger.info(f"åˆ†æ®µçµæœ: {len(segments)} å€‹æ®µè½")
        for i, segment in enumerate(segments):
            logger.info(f"æ®µè½ {i+1}: {len(segment.get('content', ''))} å­—å…ƒ")
        
        # é©—è­‰åˆ†æ®µçµæœ
        assert segments is not None, "åˆ†æ®µçµæœä¸æ‡‰ç‚º None"
        assert len(segments) > 0, "æ‡‰è©²è‡³å°‘æœ‰ä¸€å€‹åˆ†æ®µ"
        assert isinstance(segments, list), "åˆ†æ®µçµæœæ‡‰ç‚ºåˆ—è¡¨æ ¼å¼"
        
    except Exception as e:
        logger.error(f"èªæ„åˆ†æ®µæ¸¬è©¦å¤±æ•—: {e}")
        assert False, f"èªæ„åˆ†æ®µæ¸¬è©¦å¤±æ•—: {e}"

def segments():
    """pytest fixture: æä¾›èªæ„åˆ†æ®µæ¸¬è©¦è³‡æ–™"""
    test_data = create_test_data()
    transcript = test_data["transcript"]
    splitter = SemanticSplitter()
    return splitter.split_text(transcript)

# === ä»¥ä¸‹ç‚ºå·²ä¿®æ­£çš„ fixture èˆ‡æ¸¬è©¦å‡½æ•¸ ===
import pytest

@pytest.fixture
def segments_fixture():
    from scripts.semantic_splitter import SemanticSplitter
    splitter = SemanticSplitter(model_name="gemma3:12b", max_segment_length=4000, overlap_length=200)
    test_data = create_test_data()
    transcript = test_data["transcript"]
    return splitter.split_text(transcript)


def test_quality_evaluator(segments_fixture):
    from scripts.segment_quality_eval import SegmentQualityEvaluator
    evaluator = SegmentQualityEvaluator(model_name="gemma3:12b")
    test_data = create_test_data()
    original_text = test_data["transcript"]
    # evaluate_segments åªéœ€ (segments, original_text)
    quality_result = evaluator.evaluate_segments(segments_fixture, original_text)
    assert quality_result is not None

def test_meeting_optimizer_integration():
    """æ¸¬è©¦æœƒè­°å„ªåŒ–å™¨æ•´åˆ"""
    logger = logging.getLogger("OptimizerIntegrationTest")
    logger.info("=== æ¸¬è©¦æœƒè­°å„ªåŒ–å™¨æ•´åˆ ===")
    
    try:
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = project_root / "test_output"
        output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å„ªåŒ–å™¨ï¼Œå•Ÿç”¨èªæ„åˆ†æ®µ
        optimizer = MeetingOptimizer(
            model_name="cwchang/llama3-taide-lx-8b-chat-alpha1:latest",
            output_dir=str(output_dir),
            enable_semantic_segmentation=True
        )
        
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        test_data = create_test_data()
        matched_data = [test_data]
        
        logger.info("é–‹å§‹åŸ·è¡Œå„ªåŒ–æµç¨‹...")
        
        # åŸ·è¡Œå„ªåŒ–ï¼ˆä½¿ç”¨è¼ƒå°‘çš„è¿­ä»£æ¬¡æ•¸é€²è¡Œæ¸¬è©¦ï¼‰
        best_minutes, best_metrics, best_score = optimizer.optimize(
            matched_data,
            max_iterations=2,
            batch_size=1
        )
        
        logger.info(f"å„ªåŒ–å®Œæˆ:")
        logger.info(f"  æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
        logger.info(f"  ç”Ÿæˆå­—æ•¸: {len(best_minutes)} å­—å…ƒ")
        
        # æª¢æŸ¥è¼¸å‡ºæ–‡ä»¶
        output_files = list(output_dir.glob("*.json"))
        logger.info(f"ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶: {len(output_files)} å€‹")
        
        # é©—è­‰çµæœ
        assert best_score > 0, f"å„ªåŒ–åˆ†æ•¸æ‡‰å¤§æ–¼0ï¼Œç•¶å‰åˆ†æ•¸: {best_score}"
        assert len(output_files) > 0, "æ‡‰è©²ç”Ÿæˆè‡³å°‘ä¸€å€‹è¼¸å‡ºæ–‡ä»¶"
        
    except Exception as e:
        logger.error(f"æœƒè­°å„ªåŒ–å™¨æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        assert False, f"æœƒè­°å„ªåŒ–å™¨æ•´åˆæ¸¬è©¦å¤±æ•—: {e}"

def test_semantic_processor():
    """æ¸¬è©¦èªæ„è™•ç†å™¨"""
    logger = logging.getLogger("SemanticProcessorTest")
    logger.info("=== æ¸¬è©¦èªæ„è™•ç†å™¨ ===")
    
    try:
        processor = SemanticMeetingProcessor()
        
        test_data = create_test_data()
        transcript = test_data["transcript"]
        
        logger.info("åŸ·è¡Œèªæ„è™•ç†æµç¨‹...")
        
        # è™•ç†é€å­—ç¨¿
        result = processor.process_transcript(
            transcript_text=transcript,
            output_dir="output/test",  # æŒ‡å®šæ¸¬è©¦è¼¸å‡ºç›®éŒ„
            enable_quality_check=True
        )
        
        if result:
            logger.info("èªæ„è™•ç†å™¨æ¸¬è©¦æˆåŠŸ")
            logger.info(f"è™•ç†çµæœ: {type(result)}")
            assert result is not None, "è™•ç†çµæœä¸æ‡‰ç‚º None"
        else:
            logger.warning("èªæ„è™•ç†å™¨è¿”å›ç©ºçµæœ")
            assert False, "èªæ„è™•ç†å™¨è¿”å›ç©ºçµæœ"
        
    except Exception as e:
        logger.error(f"èªæ„è™•ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
        return False, ""

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    logger = setup_logging()
    logger.info("é–‹å§‹èªæ„åˆ†æ®µåŠŸèƒ½æ•´åˆæ¸¬è©¦")
    
    test_results = {}
    
    # æ¸¬è©¦1: èªæ„åˆ†æ®µå™¨
    try:
        test_semantic_splitter()
        test_results["semantic_splitter"] = True
        logger.info("âœ“ èªæ„åˆ†æ®µå™¨æ¸¬è©¦é€šé")
    except Exception as e:
        test_results["semantic_splitter"] = False
        logger.error(f"âœ— èªæ„åˆ†æ®µå™¨æ¸¬è©¦å¤±æ•—: {e}")
    
    # æ¸¬è©¦2: å“è³ªè©•ä¼°å™¨
    try:
        # ä½¿ç”¨ fixture ç²å–æ¸¬è©¦æ•¸æ“š
        from scripts.semantic_splitter import SemanticSplitter
        splitter = SemanticSplitter(model_name="gemma3:12b", max_segment_length=4000, overlap_length=200)
        test_data = create_test_data()
        transcript = test_data["transcript"]
        segments = splitter.split_text(transcript)
        
        from scripts.segment_quality_eval import SegmentQualityEvaluator
        evaluator = SegmentQualityEvaluator(model_name="gemma3:12b")
        quality_result = evaluator.evaluate_segments(segments, transcript)
        test_results["quality_evaluator"] = quality_result is not None
        logger.info("âœ“ å“è³ªè©•ä¼°å™¨æ¸¬è©¦é€šé")
    except Exception as e:
        test_results["quality_evaluator"] = False
        logger.error(f"âœ— å“è³ªè©•ä¼°å™¨æ¸¬è©¦å¤±æ•—: {e}")
    
    # æ¸¬è©¦3: èªæ„è™•ç†å™¨
    try:
        test_semantic_processor()
        test_results["semantic_processor"] = True
        logger.info("âœ“ èªæ„è™•ç†å™¨æ¸¬è©¦é€šé")
    except Exception as e:
        test_results["semantic_processor"] = False
        logger.error(f"âœ— èªæ„è™•ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
    
    # æ¸¬è©¦4: æœƒè­°å„ªåŒ–å™¨æ•´åˆ
    try:
        test_meeting_optimizer_integration()
        test_results["optimizer_integration"] = True
        logger.info("âœ“ æœƒè­°å„ªåŒ–å™¨æ•´åˆæ¸¬è©¦é€šé")
    except Exception as e:
        test_results["optimizer_integration"] = False
        logger.error(f"âœ— æœƒè­°å„ªåŒ–å™¨æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
    
    # è¼¸å‡ºæ¸¬è©¦çµæœ
    logger.info("\n=== æ¸¬è©¦çµæœç¸½çµ ===")
    for test_name, result in test_results.items():
        status = "âœ“ é€šé" if result else "âœ— å¤±æ•—"
        logger.info(f"{test_name}: {status}")
    
    all_passed = all(test_results.values())
    if all_passed:
        logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼èªæ„åˆ†æ®µåŠŸèƒ½æ•´åˆæˆåŠŸ")
        return 0
    else:
        logger.error("âŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç›¸é—œæ¨¡çµ„")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
