#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœƒè­°è¨˜éŒ„å„ªåŒ–ç³»çµ± - æ¨¡å‹é…ç½®æ¸¬è©¦èˆ‡è©•ä¼°å·¥å…·
åŸºæ–¼ MODEL_OPTIMIZATION_ANALYSIS.md çš„å»ºè­°æ–¹æ¡ˆé€²è¡Œæ¸¬è©¦
"""

import os
import json
import time
import logging
import argparse
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®è³‡æ–™çµæ§‹"""
    name: str
    generation_model: str
    optimization_model: str
    embedding_model: str = "nomic-embed-text:latest"
    description: str = ""
    expected_improvements: Optional[Dict[str, Any]] = None  # æ”¹ç‚º Optional å’Œ Any é¡å‹

class ModelConfigurationTester:
    """æ¨¡å‹é…ç½®æ¸¬è©¦å™¨"""
    
    def __init__(self, output_dir: str = "model_config_tests"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­ç½®æ—¥èªŒ
        self.logger = self._setup_logger()
        
        # å®šç¾©æ¸¬è©¦é…ç½®
        self.test_configs = self._define_test_configurations()
        
    def _setup_logger(self) -> logging.Logger:
        """è¨­ç½®æ—¥èªŒ"""
        logger = logging.getLogger("ModelConfigTester")
        logger.setLevel(logging.INFO)
        
        if logger.hasHandlers():
            logger.handlers.clear()
            
        # æ–‡ä»¶è™•ç†å™¨
        log_file = self.output_dir / "model_config_test.log"
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setLevel(logging.DEBUG)
        
        # æ§åˆ¶å°è™•ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # æ ¼å¼è¨­ç½®
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
        
    def _define_test_configurations(self) -> List[ModelConfig]:
        """å®šç¾©æ¸¬è©¦æ¨¡å‹é…ç½®
        
        åŸºæ–¼ MODEL_OPTIMIZATION_ANALYSIS.md ä¸­çš„æ¨¡å‹è¦æ ¼æ¯”è¼ƒè¡¨å’Œä¸‰å€‹å„ªåŒ–æ–¹æ¡ˆ
        """
        return [
            # æ–¹æ¡ˆä¸€ï¼šé›™Gemmaæ¶æ§‹ï¼ˆæ¨è–¦ï¼‰â­â­â­â­â­
            ModelConfig(
                name="dual_gemma_recommended",
                generation_model="gemma3:12b",
                optimization_model="gemma3:12b",
                embedding_model="nomic-embed-text:latest",
                description="é›™Gemmaæ¶æ§‹ - 12Båƒæ•¸ï¼Œ128Kä¸Šä¸‹æ–‡ï¼Œçµ±ä¸€å¼·æ¨ç†èƒ½åŠ›ï¼Œé‡åŒ–æ„ŸçŸ¥è¨“ç·´",
                expected_improvements={
                    "quality_improvement": 0.15,  # +15%
                    "stability_sigma": 0.05,      # Ïƒ<0.05
                    "avg_iterations": 25,          # 25è¼ª
                    "processing_time": 120,        # 120åˆ†é˜
                    "context_length": 128000,      # 128Kä¸Šä¸‹æ–‡
                    "model_rating": 5,             # â­â­â­â­â­
                    "technical_advantages": ["å¼·æ¨ç†èƒ½åŠ›", "é•·ä¸Šä¸‹æ–‡", "è¨˜æ†¶é«”æ•ˆç‡", "å¤šæ¨¡æ…‹æ”¯æŒ"]
                }
            ),
            
            # æ–¹æ¡ˆäºŒï¼šæ··åˆæœ€ä½³åŒ–æ¶æ§‹ â­â­â­â­
            ModelConfig(
                name="hybrid_llama_gemma",
                generation_model="llama3.1:8b",
                optimization_model="gemma3:12b",
                embedding_model="nomic-embed-text:latest",
                description="æ··åˆæ¶æ§‹ - Llama3.1(128Kä¸Šä¸‹æ–‡+å·¥å…·ä½¿ç”¨) + Gemma3:12b(å¼·æ¨ç†)",
                expected_improvements={
                    "quality_improvement": 0.12,  # +12%
                    "stability_sigma": 0.08,      # Ïƒ<0.08
                    "avg_iterations": 30,          # 30è¼ª
                    "processing_time": 135,        # 135åˆ†é˜
                    "context_length": 128000,      # 128Kä¸Šä¸‹æ–‡
                    "model_rating": 4,             # â­â­â­â­
                    "technical_advantages": ["è¶…é•·ä¸Šä¸‹æ–‡", "å·¥å…·ä½¿ç”¨èƒ½åŠ›", "å¤šèªè¨€æ”¯æŒ", "å¼·æ¨ç†å„ªåŒ–"]
                }
            ),
            
            # æ–¹æ¡ˆä¸‰ï¼šç¹ä¸­å„ªåŒ–æ¶æ§‹ â­â­â­â­
            ModelConfig(
                name="taide_optimized_chinese",
                generation_model="cwchang/llama3-taide-lx-8b-chat-alpha1:latest",
                optimization_model="gemma3:12b",
                embedding_model="nomic-embed-text:latest",
                description="ç¹ä¸­å„ªåŒ–æ¶æ§‹ - TAIDEç¹é«”ä¸­æ–‡ç‰¹å„ª + Gemma3:12bæ¨ç† + å‡ç´šembedding",
                expected_improvements={
                    "quality_improvement": 0.08,  # +8%
                    "stability_sigma": 0.12,      # Ïƒ<0.12
                    "avg_iterations": 35,          # 35è¼ª
                    "processing_time": 140,        # 140åˆ†é˜
                    "context_length": 8192,        # 8Kä¸Šä¸‹æ–‡
                    "model_rating": 4,             # â­â­â­â­
                    "technical_advantages": ["ç¹é«”ä¸­æ–‡ç‰¹å„ª", "ä¸­ç¿»è‹±å„ªç•°", "æœ€å°è®Šæ›´é¢¨éšª", "ç©©å®šåŸºç¤"]
                }
            ),
            
            # è¼•é‡åŒ–æ–¹æ¡ˆï¼šGemma3:4bæ¶æ§‹ â­â­â­
            ModelConfig(
                name="lightweight_gemma",
                generation_model="gemma3:4b",
                optimization_model="gemma3:12b",
                embedding_model="nomic-embed-text:latest",
                description="è¼•é‡æ¶æ§‹ - Gemma3:4bç”Ÿæˆ + Gemma3:12bå„ªåŒ–ï¼Œè³‡æºæ•ˆç‡é«˜",
                expected_improvements={
                    "quality_improvement": 0.06,  # +6%
                    "stability_sigma": 0.10,      # Ïƒ<0.10
                    "avg_iterations": 40,          # 40è¼ª
                    "processing_time": 100,        # 100åˆ†é˜
                    "context_length": 128000,      # 128Kä¸Šä¸‹æ–‡
                    "model_rating": 3,             # â­â­â­
                    "technical_advantages": ["è¼•é‡è¨­è¨ˆ", "è³‡æºæ•ˆç‡", "å¿«é€Ÿè™•ç†", "128Kä¸Šä¸‹æ–‡"]
                }
            ),
            
            # åŸºæº–ç·šï¼šç•¶å‰ç³»çµ±é…ç½®
            ModelConfig(
                name="current_baseline",
                generation_model="cwchang/llama3-taide-lx-8b-chat-alpha1:latest",
                optimization_model="gemma3:12b",
                embedding_model="current",  # ç•¶å‰embeddingç³»çµ±
                description="ç•¶å‰ç³»çµ±é…ç½® - åŸºæº–ç·šæ¸¬è©¦ï¼ˆTAIDE + Gemma3:12b + èˆŠembeddingï¼‰",
                expected_improvements={
                    "quality_improvement": 0.00,  # åŸºæº–ç·š
                    "stability_sigma": 0.2078,    # ç•¶å‰Ïƒ
                    "avg_iterations": 63,          # ç•¶å‰å¹³å‡
                    "processing_time": 145.8,      # ç•¶å‰æ™‚é–“
                    "context_length": 8192,        # 8Kä¸Šä¸‹æ–‡
                    "model_rating": 3,             # ç•¶å‰åŸºæº–
                    "technical_advantages": ["å·²é©—è­‰ç©©å®š", "ç¹é«”ä¸­æ–‡å„ªå‹¢", "ç¾æœ‰é…ç½®"]
                }
            )
        ]
    
    def check_model_availability(self, model_name: str) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                available_models = result.stdout
                return model_name in available_models
            else:
                self.logger.error(f"æª¢æŸ¥æ¨¡å‹æ™‚å‡ºéŒ¯: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"æª¢æŸ¥æ¨¡å‹ {model_name} å¯ç”¨æ€§æ™‚å‡ºéŒ¯: {e}")
            return False
    
    def download_model_if_needed(self, model_name: str) -> bool:
        """å¦‚æœéœ€è¦ï¼Œä¸‹è¼‰æ¨¡å‹"""
        if self.check_model_availability(model_name):
            self.logger.info(f"æ¨¡å‹ {model_name} å·²å­˜åœ¨")
            return True
            
        self.logger.info(f"æ­£åœ¨ä¸‹è¼‰æ¨¡å‹ {model_name}...")
        try:
            result = subprocess.run(
                ["ollama", "pull", model_name],
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é˜è¶…æ™‚
            )
            
            if result.returncode == 0:
                self.logger.info(f"æ¨¡å‹ {model_name} ä¸‹è¼‰æˆåŠŸ")
                return True
            else:
                self.logger.error(f"æ¨¡å‹ {model_name} ä¸‹è¼‰å¤±æ•—: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"æ¨¡å‹ {model_name} ä¸‹è¼‰è¶…æ™‚")
            return False
        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰æ¨¡å‹ {model_name} æ™‚å‡ºéŒ¯: {e}")
            return False
    
    def run_baseline_test(self, config: ModelConfig, max_iterations: int = 10) -> Dict[str, Any]:
        """åŸ·è¡ŒåŸºæº–æ¸¬è©¦"""
        self.logger.info(f"é–‹å§‹æ¸¬è©¦é…ç½®: {config.name}")
        self.logger.info(f"æè¿°: {config.description}")
        
        # æª¢æŸ¥ä¸¦ä¸‹è¼‰å¿…è¦æ¨¡å‹
        models_to_check = [config.generation_model, config.optimization_model]
        
        for model_name in models_to_check:
            if model_name == "current":  # è·³éç•¶å‰embeddingæ¨™è¨˜
                continue
                
            if not self.download_model_if_needed(model_name):
                self.logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ {model_name}ï¼Œè·³éæ­¤é…ç½®æ¸¬è©¦")
                return {
                    "config_name": config.name,
                    "status": "failed",
                    "error": f"æ¨¡å‹ {model_name} ä¸å¯ç”¨",
                    "timestamp": datetime.now().isoformat()
                }
        
        # åŸ·è¡Œå„ªåŒ–æ¸¬è©¦
        test_start_time = time.time()
        
        try:
            # æ§‹å»ºæ¸¬è©¦å‘½ä»¤
            cmd = [
                "python", "scripts/iterative_optimizer.py",
                "--transcript-dir", "data/transcript",
                "--model", config.generation_model,
                "--optimization-model", config.optimization_model,
                "--max-iterations", str(max_iterations),
                "--quality-threshold", "0.8"
            ]
            
            self.logger.info(f"åŸ·è¡Œæ¸¬è©¦å‘½ä»¤: {' '.join(cmd)}")
            
            # åŸ·è¡Œæ¸¬è©¦
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ™‚è¶…æ™‚
            )
            
            test_duration = time.time() - test_start_time
            
            if result.returncode == 0:
                self.logger.info(f"é…ç½® {config.name} æ¸¬è©¦å®Œæˆï¼Œè€—æ™‚ {test_duration:.2f} ç§’")
                
                # è§£ææ¸¬è©¦çµæœ
                test_results = self._parse_test_results(config.name, result.stdout)
                test_results.update({
                    "config_name": config.name,
                    "status": "success",
                    "test_duration": test_duration,
                    "timestamp": datetime.now().isoformat(),
                    "expected_improvements": config.expected_improvements
                })
                
                return test_results
            else:
                self.logger.error(f"é…ç½® {config.name} æ¸¬è©¦å¤±æ•—: {result.stderr}")
                return {
                    "config_name": config.name,
                    "status": "failed",
                    "error": result.stderr,
                    "test_duration": test_duration,
                    "timestamp": datetime.now().isoformat()
                }
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"é…ç½® {config.name} æ¸¬è©¦è¶…æ™‚")
            return {
                "config_name": config.name,
                "status": "timeout",
                "test_duration": time.time() - test_start_time,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            self.logger.error(f"æ¸¬è©¦é…ç½® {config.name} æ™‚å‡ºéŒ¯: {e}")
            return {
                "config_name": config.name,
                "status": "error",
                "error": str(e),
                "test_duration": time.time() - test_start_time,
                "timestamp": datetime.now().isoformat()
            }
    
    def _parse_test_results(self, config_name: str, stdout: str) -> Dict[str, Any]:
        """è§£ææ¸¬è©¦çµæœ"""
        # é€™è£¡å¯¦ç¾çµæœè§£æé‚è¼¯
        # æš«æ™‚è¿”å›åŸºæœ¬è³‡è¨Š
        return {
            "parsing_status": "basic",
            "stdout_length": len(stdout),
            "note": "è©³ç´°çµæœè§£æå¾…å¯¦ç¾"
        }
    
    def run_comprehensive_test(self, max_iterations: int = 10) -> Dict[str, Any]:
        """åŸ·è¡Œå…¨é¢é…ç½®æ¸¬è©¦"""
        self.logger.info("é–‹å§‹åŸ·è¡Œå…¨é¢æ¨¡å‹é…ç½®æ¸¬è©¦")
        self.logger.info(f"æ¸¬è©¦ {len(self.test_configs)} å€‹é…ç½®")
        
        all_results = {
            "test_start_time": datetime.now().isoformat(),
            "test_parameters": {
                "max_iterations": max_iterations,
                "configs_tested": len(self.test_configs)
            },
            "config_results": {},
            "summary": {}
        }
        
        # é€ä¸€æ¸¬è©¦æ¯å€‹é…ç½®
        for config in self.test_configs:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"æ¸¬è©¦é…ç½®: {config.name}")
            self.logger.info(f"{'='*60}")
            
            config_result = self.run_baseline_test(config, max_iterations)
            all_results["config_results"][config.name] = config_result
            
            # ç°¡çŸ­ä¼‘æ¯ï¼Œé¿å…ç³»çµ±éè¼‰
            time.sleep(10)
        
        # ç”Ÿæˆæ¸¬è©¦ç¸½çµ
        all_results["summary"] = self._generate_test_summary(all_results["config_results"])
        all_results["test_end_time"] = datetime.now().isoformat()
        
        # ä¿å­˜çµæœ
        results_file = self.output_dir / f"comprehensive_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æ¸¬è©¦å®Œæˆï¼Œçµæœå·²ä¿å­˜è‡³: {results_file}")
        return all_results
    
    def _generate_test_summary(self, config_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦ç¸½çµ"""
        summary = {
            "total_configs": len(config_results),
            "successful_tests": 0,
            "failed_tests": 0,
            "timeout_tests": 0,
            "recommendations": []
        }
        
        for config_name, result in config_results.items():
            status = result.get("status", "unknown")
            if status == "success":
                summary["successful_tests"] += 1
            elif status == "failed" or status == "error":
                summary["failed_tests"] += 1
            elif status == "timeout":
                summary["timeout_tests"] += 1
        
        # ç”Ÿæˆå»ºè­°
        if summary["successful_tests"] > 0:
            summary["recommendations"].append("è‡³å°‘æœ‰ä¸€å€‹é…ç½®æ¸¬è©¦æˆåŠŸï¼Œå¯ä»¥é€²è¡Œä¸‹ä¸€æ­¥åˆ†æ")
        
        if summary["failed_tests"] > 0:
            summary["recommendations"].append("éƒ¨åˆ†é…ç½®æ¸¬è©¦å¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§å’Œç³»çµ±è³‡æº")
        
        return summary

    def generate_model_specs_comparison(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹è¦æ ¼æ¯”è¼ƒå ±å‘Š"""
        self.logger.info("ç”Ÿæˆæ¨¡å‹è¦æ ¼æ¯”è¼ƒå ±å‘Š")
        
        comparison_data = {
            "report_timestamp": datetime.now().isoformat(),
            "model_specifications": {},
            "ranking_analysis": {},
            "recommendations": {}
        }
        
        # ç‚ºæ¯å€‹é…ç½®ç”Ÿæˆè¦æ ¼åˆ†æ
        for config in self.test_configs:
            specs = {
                "configuration_name": config.name,
                "generation_model": config.generation_model,
                "optimization_model": config.optimization_model,
                "embedding_model": config.embedding_model,
                "description": config.description,
                "expected_improvements": config.expected_improvements or {},
                "model_rating": config.expected_improvements.get("model_rating", 0) if config.expected_improvements else 0,
                "context_length": config.expected_improvements.get("context_length", 0) if config.expected_improvements else 0,
                "technical_advantages": config.expected_improvements.get("technical_advantages", []) if config.expected_improvements else []
            }
            comparison_data["model_specifications"][config.name] = specs
        
        # ç”Ÿæˆæ’ååˆ†æ
        rankings = sorted(
            self.test_configs,
            key=lambda x: x.expected_improvements.get("quality_improvement", 0) if x.expected_improvements else 0,
            reverse=True
        )
        
        comparison_data["ranking_analysis"] = {
            "by_quality_improvement": [
                {
                    "rank": i+1,
                    "config_name": config.name,
                    "expected_improvement": config.expected_improvements.get("quality_improvement", 0) if config.expected_improvements else 0,
                    "rating": config.expected_improvements.get("model_rating", 0) if config.expected_improvements else 0
                }
                for i, config in enumerate(rankings)
            ],
            "by_stability": sorted([
                {
                    "config_name": config.name,
                    "expected_sigma": config.expected_improvements.get("stability_sigma", 1.0) if config.expected_improvements else 1.0,
                    "stability_score": 1.0 / (config.expected_improvements.get("stability_sigma", 1.0) + 0.01) if config.expected_improvements else 0.01
                }
                for config in self.test_configs
            ], key=lambda x: x["stability_score"], reverse=True)
        }
        
        # ç”Ÿæˆå»ºè­°
        best_config = rankings[0] if rankings else None
        comparison_data["recommendations"] = {
            "top_recommendation": best_config.name if best_config else "ç„¡æ³•ç¢ºå®š",
            "reasoning": f"åŸºæ–¼æ¨¡å‹è¦æ ¼åˆ†æï¼Œ{best_config.name}å…·æœ‰æœ€é«˜çš„å“è³ªæ”¹å–„æ½›åŠ›" if best_config else "éœ€è¦æ›´å¤šæ•¸æ“š",
            "implementation_priority": [
                f"ç«‹å³æ¸¬è©¦: {rankings[0].name}" if len(rankings) > 0 else "",
                f"æ¬¡è¦é¸æ“‡: {rankings[1].name}" if len(rankings) > 1 else "",
                f"å‚™ç”¨æ–¹æ¡ˆ: {rankings[2].name}" if len(rankings) > 2 else ""
            ],
            "risk_assessment": {
                "high_performance": [config.name for config in self.test_configs if config.expected_improvements and config.expected_improvements.get("model_rating", 0) >= 5],
                "moderate_risk": [config.name for config in self.test_configs if config.expected_improvements and config.expected_improvements.get("model_rating", 0) == 4],
                "conservative_choice": [config.name for config in self.test_configs if config.expected_improvements and config.expected_improvements.get("model_rating", 0) <= 3]
            }
        }
        
        # ä¿å­˜æ¯”è¼ƒå ±å‘Š
        report_file = self.output_dir / f"model_specs_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æ¨¡å‹è¦æ ¼æ¯”è¼ƒå ±å‘Šå·²ç”Ÿæˆ: {report_file}")
        return comparison_data

    def run_quick_validation_test(self, config_name: str) -> Dict[str, Any]:
        """åŸ·è¡Œå¿«é€Ÿé©—è­‰æ¸¬è©¦ï¼ˆåƒ…æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§å’ŒåŸºæœ¬åŠŸèƒ½ï¼‰"""
        config = next((c for c in self.test_configs if c.name == config_name), None)
        if not config:
            return {"error": f"æ‰¾ä¸åˆ°é…ç½®: {config_name}"}
        
        self.logger.info(f"åŸ·è¡Œå¿«é€Ÿé©—è­‰æ¸¬è©¦: {config_name}")
        
        validation_result = {
            "config_name": config_name,
            "timestamp": datetime.now().isoformat(),
            "model_availability": {},
            "basic_functionality": {},
            "validation_status": "unknown"
        }
        
        # æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§
        models_to_check = [config.generation_model, config.optimization_model]
        if config.embedding_model != "current":
            models_to_check.append(config.embedding_model)
        
        all_available = True
        for model in models_to_check:
            available = self.check_model_availability(model)
            validation_result["model_availability"][model] = available
            if not available:
                all_available = False
                self.logger.warning(f"æ¨¡å‹ {model} ä¸å¯ç”¨")
        
        if all_available:
            validation_result["validation_status"] = "models_ready"
            self.logger.info(f"é…ç½® {config_name} çš„æ‰€æœ‰æ¨¡å‹éƒ½å¯ç”¨")
        else:
            validation_result["validation_status"] = "models_missing"
            self.logger.error(f"é…ç½® {config_name} ç¼ºå°‘å¿…è¦æ¨¡å‹")
        
        return validation_result

def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description="æœƒè­°è¨˜éŒ„å„ªåŒ–ç³»çµ± - æ¨¡å‹é…ç½®æ¸¬è©¦å·¥å…·")
    parser.add_argument("--mode", choices=["specs", "validate", "quick", "full"], default="specs",
                       help="æ¸¬è©¦æ¨¡å¼: specs=ç”Ÿæˆè¦æ ¼æ¯”è¼ƒ, validate=é©—è­‰æ¨¡å‹å¯ç”¨æ€§, quick=å¿«é€Ÿæ¸¬è©¦, full=å®Œæ•´æ¸¬è©¦")
    parser.add_argument("--config", type=str, help="æŒ‡å®šæ¸¬è©¦çš„é…ç½®åç¨±")
    parser.add_argument("--max-iterations", type=int, default=5, help="æœ€å¤§ç–Šä»£æ¬¡æ•¸ï¼ˆç”¨æ–¼quick/fullæ¨¡å¼ï¼‰")
    parser.add_argument("--output-dir", type=str, default="model_config_tests", help="è¼¸å‡ºç›®éŒ„")
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¸¬è©¦å™¨
    tester = ModelConfigurationTester(output_dir=args.output_dir)
    
    try:
        if args.mode == "specs":
            # ç”Ÿæˆæ¨¡å‹è¦æ ¼æ¯”è¼ƒå ±å‘Š
            print("ğŸ” ç”Ÿæˆæ¨¡å‹è¦æ ¼æ¯”è¼ƒå ±å‘Š...")
            comparison = tester.generate_model_specs_comparison()
            print(f"âœ… è¦æ ¼æ¯”è¼ƒå ±å‘Šå·²ç”Ÿæˆï¼Œå…±åˆ†æ {len(comparison['model_specifications'])} å€‹é…ç½®")
            
            # é¡¯ç¤ºæ¨è–¦é…ç½®
            if comparison["recommendations"]["top_recommendation"] != "ç„¡æ³•ç¢ºå®š":
                print(f"ğŸš€ æ¨è–¦é…ç½®: {comparison['recommendations']['top_recommendation']}")
                print(f"ğŸ“ æ¨è–¦ç†ç”±: {comparison['recommendations']['reasoning']}")
                
                # é¡¯ç¤ºå¯¦æ–½å„ªå…ˆç´š
                print("\nğŸ“‹ å¯¦æ–½å„ªå…ˆç´š:")
                for priority in comparison["recommendations"]["implementation_priority"]:
                    if priority:  # åªé¡¯ç¤ºéç©ºçš„å„ªå…ˆç´š
                        print(f"   â€¢ {priority}")
        
        elif args.mode == "validate":
            # é©—è­‰æ‰€æœ‰é…ç½®çš„æ¨¡å‹å¯ç”¨æ€§
            print("ğŸ”§ é©—è­‰æ¨¡å‹å¯ç”¨æ€§...")
            validation_results = {}
            
            configs_to_test = [args.config] if args.config else [c.name for c in tester.test_configs]
            
            for config_name in configs_to_test:
                print(f"\nğŸ“‹ é©—è­‰é…ç½®: {config_name}")
                result = tester.run_quick_validation_test(config_name)
                validation_results[config_name] = result
                
                status = result.get("validation_status", "unknown")
                if status == "models_ready":
                    print(f"âœ… {config_name}: æ‰€æœ‰æ¨¡å‹å¯ç”¨")
                else:
                    print(f"âŒ {config_name}: ç¼ºå°‘å¿…è¦æ¨¡å‹")
                    for model, available in result.get("model_availability", {}).items():
                        if not available:
                            print(f"   âš ï¸  {model} ä¸å¯ç”¨")
            
            # ä¿å­˜é©—è­‰çµæœ
            validation_file = Path(args.output_dir) / f"validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(validation_file, 'w', encoding='utf-8') as f:
                json.dump(validation_results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“Š é©—è­‰çµæœå·²ä¿å­˜: {validation_file}")
        
        elif args.mode == "quick":
            # å¿«é€ŸåŸºæº–æ¸¬è©¦
            print(f"âš¡ åŸ·è¡Œå¿«é€ŸåŸºæº–æ¸¬è©¦ï¼ˆ{args.max_iterations}æ¬¡ç–Šä»£ï¼‰...")
            
            if args.config:
                config = next((c for c in tester.test_configs if c.name == args.config), None)
                if config:
                    result = tester.run_baseline_test(config, max_iterations=args.max_iterations)
                    print(f"âœ… é…ç½® {args.config} æ¸¬è©¦å®Œæˆ")
                    print(f"ğŸ“Š ç‹€æ…‹: {result.get('status', 'unknown')}")
                else:
                    print(f"âŒ æ‰¾ä¸åˆ°é…ç½®: {args.config}")
            else:
                print("âŒ å¿«é€Ÿæ¸¬è©¦æ¨¡å¼éœ€è¦æŒ‡å®š --config åƒæ•¸")
                print("ğŸ’¡ å¯ç”¨é…ç½®:")
                for config in tester.test_configs:
                    print(f"   â€¢ {config.name}: {config.description}")
        
        elif args.mode == "full":
            # å®Œæ•´æ¸¬è©¦
            print(f"ğŸš€ åŸ·è¡Œå®Œæ•´æ¨¡å‹é…ç½®æ¸¬è©¦ï¼ˆ{args.max_iterations}æ¬¡ç–Šä»£ï¼‰...")
            results = tester.run_comprehensive_test(max_iterations=args.max_iterations)
            
            print("\nğŸ“Š æ¸¬è©¦ç¸½çµ:")
            summary = results.get("summary", {})
            print(f"   ç¸½é…ç½®æ•¸: {summary.get('total_configs', 0)}")
            print(f"   æˆåŠŸæ¸¬è©¦: {summary.get('successful_tests', 0)}")
            print(f"   å¤±æ•—æ¸¬è©¦: {summary.get('failed_tests', 0)}")
            print(f"   è¶…æ™‚æ¸¬è©¦: {summary.get('timeout_tests', 0)}")
            
            # é¡¯ç¤ºæ¨è–¦
            recommendations = summary.get("recommendations", [])
            if recommendations:
                print("\nğŸ’¡ ç³»çµ±å»ºè­°:")
                for rec in recommendations[:3]:  # é¡¯ç¤ºå‰ä¸‰å€‹å»ºè­°
                    print(f"   â€¢ {rec}")
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ¶ä¸­æ–·æ¸¬è©¦")
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­å‡ºéŒ¯: {e}")
        tester.logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")
    
    print("\nğŸ æ¸¬è©¦ç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()
